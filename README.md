# Multi-threaded Web Crawler and Data Management Tool

## Introduction

This Python tool suite is designed for web data scraping and management, consisting of two main scripts. The first script is a multi-threaded web crawler for scraping web page links and saving the data into Excel files. The second script is for generating crawler parameters and saving them into an Excel file.

## Features

1. **Multi-threaded Crawler**: Concurrently scrape web data to improve efficiency.
2. **Progress Monitoring**: Real-time display of crawler progress through a Graphical User Interface (GUI).
3. **Data Management**: Save scraped data into Excel files for further processing.
4. **Parameter Generation**: Automatically generate parameters required for the crawler and save them into an Excel file.

## Prerequisites

- Python 3.x
- Selenium
- Pandas
- Openpyxl
- Tkinter
- Edgedriver

## Installation

1. Ensure that the Python environment is installed.
2. Install necessary libraries using pip:
pip install selenium pandas openpyxl tkinter
3. Download the corresponding Webdriver for your browser and ensure its path is added to the system environment variables.

## Usage

### Multi-threaded Crawler Script

1. **Prepare the Excel Parameter File**:
- Make sure you have Excel or a compatible spreadsheet software installed.
- Create a new Excel file or use the parameter file generated by the script.
- The Excel file should include columns such as `course`, `country`, `start_url`, `pages`, `file_name`, `path`, `crawled`.

2. **Configure the Excel File**:
- Fill in the details for each crawling task in the Excel file, including the course name, country, start URL, number of pages to crawl, file name for saving, data save path, and a flag indicating whether it has been crawled.

3. **Run the Crawler Script**:
- Open a command-line tool (such as CMD, Terminal, or PowerShell).
- Navigate to the directory where the script is located.
- Execute the command `python crawler_script_name.py` to start the crawler.

4. **Monitor Crawler Progress**:
- After launching the crawler, a GUI window will pop up showing the real-time progress of each crawler thread.
- Progress information includes the number of pages crawled, total number of pages, and completion percentage.

5. **Data Saving**:
- The crawled data will be automatically saved to the specified Excel file.
- Once each crawler thread is completed, the progress bar will show 100%.

### Parameter Generation Script

1. **Input Information**:
- After running the script, input the names of the countries you want to crawl (in English), separated by commas for multiple countries.
- Input the name of the person, which will be used to organize the folders where the data is saved.

2. **Generate Parameters**:
- The script will automatically generate crawler parameters based on the input countries and person's name.
- Parameters include specific search keywords and URLs for each country, as well as the number of pages to crawl.

3. **Save Parameters to Excel**:
- The generated parameters will be automatically appended to the specified Excel file.
- If the Excel file does not exist, the script will create a new file and add the necessary column headers.

4. **Check and Run the Crawler**:
- Check if the parameters in the Excel file are correct.
- Use the multi-threaded crawler script to scrape data based on these parameters.

## Scope and Scenarios

These scripts are primarily used for web data scraping and parameter management, suitable for scenarios such as:

- **Market Research**: Scrape market information from specific countries or regions, such as contact information for distributors, importers, and agents.
- **Academic Research**: Collect publicly available data on the web for statistical analysis or literature reviews.
- **Business Intelligence**: Monitor the online presence and information dissemination of competitors or related industries.
- **Data Collection**: Gather initial datasets for databases or data warehouses for further processing and analysis.

**Note**: When using web crawling techniques, always adhere to the `robots.txt` file of the target website, respect the website's crawling rules, and avoid causing unnecessary load on the website. Additionally, ensure that your actions comply with local laws and regulations and internet ethical standards.

## License

This project is licensed under the [MIT License](LICENSE).

---

## PS
This project also consists of other scripts that have already been put into use, including data cleaning, data statistics, and one click opening of links. Due to limited free time, please wait for the next update
